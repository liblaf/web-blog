const e=JSON.parse('{"key":"v-c056fdb2","path":"/course-work/hpc/2022/06/21/pa4-%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98/","title":"PA4: 稀疏矩阵-矩阵乘","lang":"en-US","frontmatter":{"lastUpdated":true,"contributors":true,"editLink":true,"category":["Course Work"],"tag":["Introduction to High Performance Computing","CUDA","SpMM"],"isOriginal":true,"date":"2022-06-21T00:00:00.000Z","permalinkPattern":"course-work/hpc/:year/:month/:day/:slug","title":"PA4: 稀疏矩阵-矩阵乘","description":"优化策略 warp divergence ref 实现中的 warp divergence 主要是因为将不同行归入一个 warp 计算, 而不同行的 NNZ 可能有很大差异, 产生 warp divergence. 因此, 只要避免将不同行划入同一 warp 即可. 因此, 令 block.x = 1, 使每个 thread block 至多处理一行数据. 如图中 ref 与 phase_1 对比, 提升有限. shared memory 在 SpMM 中, 稀疏矩阵的一个元素代表了对于稠密矩阵的一行的访问. 因此可以将稀疏矩阵的一部分缓存在 shared memory 中, 以减少重复从 global memory 中读取稀疏矩阵.","head":[["meta",{"property":"og:url","content":"https://blog.liblaf.me/course-work/hpc/2022/06/21/pa4-%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98/"}],["meta",{"property":"og:site_name","content":"Blog"}],["meta",{"property":"og:title","content":"PA4: 稀疏矩阵-矩阵乘"}],["meta",{"property":"og:description","content":"优化策略 warp divergence ref 实现中的 warp divergence 主要是因为将不同行归入一个 warp 计算, 而不同行的 NNZ 可能有很大差异, 产生 warp divergence. 因此, 只要避免将不同行划入同一 warp 即可. 因此, 令 block.x = 1, 使每个 thread block 至多处理一行数据. 如图中 ref 与 phase_1 对比, 提升有限. shared memory 在 SpMM 中, 稀疏矩阵的一个元素代表了对于稠密矩阵的一行的访问. 因此可以将稀疏矩阵的一部分缓存在 shared memory 中, 以减少重复从 global memory 中读取稀疏矩阵."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2023-02-24T05:36:48.000Z"}],["meta",{"property":"article:tag","content":"Introduction to High Performance Computing"}],["meta",{"property":"article:tag","content":"CUDA"}],["meta",{"property":"article:tag","content":"SpMM"}],["meta",{"property":"article:published_time","content":"2022-06-21T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-02-24T05:36:48.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"PA4: 稀疏矩阵-矩阵乘\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2022-06-21T00:00:00.000Z\\",\\"dateModified\\":\\"2023-02-24T05:36:48.000Z\\",\\"author\\":[]}"]]},"headers":[{"level":2,"title":"优化策略","slug":"优化策略","link":"#优化策略","children":[{"level":3,"title":"warp divergence","slug":"warp-divergence","link":"#warp-divergence","children":[]},{"level":3,"title":"shared memory","slug":"shared-memory","link":"#shared-memory","children":[]},{"level":3,"title":"load imbalance","slug":"load-imbalance","link":"#load-imbalance","children":[]}]},{"level":2,"title":"Performance","slug":"performance","link":"#performance","children":[{"level":3,"title":"kLen = 32","slug":"klen-32","link":"#klen-32","children":[]},{"level":3,"title":"kLen = 256","slug":"klen-256","link":"#klen-256","children":[]}]}],"git":{"createdTime":1677217008000,"updatedTime":1677217008000,"contributors":[{"name":"Qin Li","email":"liblaf@outlook.com","commits":1}]},"readingTime":{"minutes":1.51,"words":454},"filePathRelative":"course-work/hpc/2022-06-21-pa4-稀疏矩阵-矩阵乘.md","localizedDate":"June 21, 2022","excerpt":"<h2> 优化策略</h2>\\n<h3> warp divergence</h3>\\n<p>ref 实现中的 warp divergence 主要是因为将不同行归入一个 warp 计算, 而不同行的 NNZ 可能有很大差异, 产生 warp divergence. 因此, 只要避免将不同行划入同一 warp 即可. 因此, 令 <code>block.x = 1</code>, 使每个 thread block 至多处理一行数据.</p>\\n<p>如图中 ref 与 phase_1 对比, 提升有限.</p>\\n<h3> shared memory</h3>\\n<p>在 SpMM 中, 稀疏矩阵的一个元素代表了对于稠密矩阵的一行的访问. 因此可以将稀疏矩阵的一部分缓存在 shared memory 中, 以减少重复从 global memory 中读取稀疏矩阵.</p>","autoDesc":true}');export{e as data};
